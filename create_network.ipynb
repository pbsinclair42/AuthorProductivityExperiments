{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, re, ast, itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# util functions\n",
    "\n",
    "# returns a list of all the abstract filenames. (no path)\n",
    "def get_abstracts(path):\n",
    "    # takes path to dir, return only files in dir with filenames consisting of numbers and ending in .abs. \n",
    "    raw = os.listdir(path)\n",
    "    regex = re.compile(r'^[0-9]+\\.abs$')\n",
    "    return [i for i in raw if regex.search(i)]\n",
    "\n",
    "# returns a list of all the years (no path)\n",
    "def get_years(path):\n",
    "    raw = os.listdir(path)\n",
    "    regex = re.compile(r'[0-9]{4}')\n",
    "    return [i for i in raw if regex.search(i)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# abstract -> list of raw names of authors\n",
    "def get_authors(filename):\n",
    "    \"\"\"\n",
    "    Given an abstract will return the list of raw author names\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        file_string = f.read()\n",
    "    try:\n",
    "        # get the line/string of authors\n",
    "        author_string = file_string.replace('Author:', 'Authors:').split('Authors: ')[1].split('\\n')[0]\n",
    "        # iteratively remove \" (....) \" from author string\n",
    "        while \"(\" in author_string:\n",
    "            start_index = author_string.find('(')\n",
    "            if \")\" in author_string:\n",
    "                end_index = author_string.find(')')\n",
    "                author_string = author_string[0:start_index] + author_string[end_index+1:]\n",
    "            # bracket didn't close, just drop the remaining string.\n",
    "            else:\n",
    "                author_string = author_string[0:start_index]\n",
    "        # split names on comma and \"and\".   \n",
    "        author_string = author_string.replace(\" and \", \", \")\n",
    "        authors_raw = author_string.split(\",\")\n",
    "        # remove any part of the raw author name that comes after more than one space. \n",
    "        for i in range(len(authors_raw)):\n",
    "            authors_raw[i] = authors_raw[i].split(\"  \")[0]\n",
    "            authors_raw[i] = authors_raw[i].split(\"\\t\")[0]\n",
    "        \n",
    "        # remove any names that contain Uni\n",
    "        authors = filter(lambda x: 'Uni' not in x, authors_raw)\n",
    "        # remove whitespaces from start and tail. \n",
    "        authors = map(lambda x: x.strip(), authors)\n",
    "        # remove empty string names\n",
    "        authors = filter(lambda x: len(x)>=1, authors)\n",
    "        \n",
    "        return authors\n",
    "    except IndexError:\n",
    "        print(\"Could not retrieve authors from file: {}\".format(filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def names_similar(name1, name2):\n",
    "    \"\"\"\n",
    "    returns true if name1 and name2 have first letter, and lastnames in common.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        lastname_pattern = re.compile(r'[A-Z][^\\s\\.]+$')\n",
    "        last_name1 = lastname_pattern.search(name1).group()\n",
    "        last_name2 = lastname_pattern.search(name2).group()\n",
    "        similar = last_name1==last_name2 and name1[0] == name2[0]\n",
    "    except AttributeError:\n",
    "        similar = False\n",
    "    return similar\n",
    "    \n",
    "\n",
    "def group_similar_names(raw_name_set):\n",
    "    \"\"\"\n",
    "    Takes a set of raw_names, and outputs a list of lists of similar names. \n",
    "    \"\"\"\n",
    "    name_objs = []\n",
    "    while(len(raw_name_set) != 0):\n",
    "        name = raw_name_set.pop()\n",
    "        names = [x for x in raw_name_set if names_similar(name, x)]\n",
    "        names.append(name)\n",
    "        name_objs.append(names)\n",
    "        # now remove names from set\n",
    "        raw_name_set = raw_name_set.difference(names)\n",
    "    return name_objs\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get distinct raw names across all abstracts for a certain year\n",
    "def get_distinct_raw_names(path):\n",
    "    \"takes path to a year dir, returns set of distinct raw names of authors\"\n",
    "    list_of_authors = list(map(get_authors, map(lambda x: path + x, get_abstracts(path))))\n",
    "    flatten = itertools.chain.from_iterable(list_of_authors)\n",
    "    set_of_authors = set(flatten)\n",
    "    return set_of_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get distinct raw names across all years. returns a list of distinct raw names in the entire dataset.\n",
    "def get_distinct_raw_names_all_years():\n",
    "    distinct_raw_names = set()\n",
    "    dirs = list(map(lambda x: 'hep-th-abs/' + x + '/', get_years(\"hep-th-abs/\")))\n",
    "    for directory in dirs: \n",
    "        raw_names = get_distinct_raw_names(directory)\n",
    "        distinct_raw_names = distinct_raw_names.union(raw_names)\n",
    "    return distinct_raw_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the list of groups of similar raw names - across all years in the dataset. \n",
    "def get_all_name_groups():\n",
    "    distinct_raw_names = get_distinct_raw_names_all_years()\n",
    "    all_name_groups = group_similar_names(distinct_raw_names)\n",
    "    return all_name_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# each list of similar names should now be assigned an ID - and write this to a text file. \n",
    "def create_author_map():\n",
    "    all_name_groups = get_all_name_groups()\n",
    "    with open(\"author_map.txt\", 'a') as f:\n",
    "        for author_id, name_lst in enumerate(all_name_groups):\n",
    "            f.write(str(author_id) + '\\t' + str(name_lst) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load author map into a dictionary from text file.\n",
    "def get_author_map():\n",
    "    authors_dict = {}\n",
    "    with open('author_map.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            author_id, raw_names = line.split('\\t')\n",
    "            raw_names = ast.literal_eval(raw_names)\n",
    "            authors_dict[author_id] = raw_names\n",
    "        return authors_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build author_map\n",
    "create_author_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "author_dict = get_author_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" now get the abstract map.  \"\"\"\n",
    "# get the author IDs for a particular abstract\n",
    "def get_abstract_author_ids(filename, author_dict):\n",
    "    authors = set(get_authors(filename))\n",
    "    authors_added = 0\n",
    "    author_ids = []\n",
    "    # check for every author if author_dict if in authors\n",
    "    for key_id, names in author_dict.iteritems():\n",
    "        # if they intersect, then add id to author_ids\n",
    "        if bool(set(names) & set(authors)):\n",
    "            author_ids.append(key_id)\n",
    "            authors_added += 1\n",
    "            # if this was the last remaining author, stop the loop.\n",
    "            if authors_added >= len(authors):\n",
    "                break\n",
    "    # now return the IDs of authors of the abstract\n",
    "    return author_ids\n",
    "\n",
    "# create the abstract map write it to a file. Schema: file_id ::: [author_ids]\n",
    "def create_abstract_map(author_dict):\n",
    "    with open(\"abstract_map.txt\", 'a') as f:\n",
    "        dirs = list(map(lambda x: 'hep-th-abs/' + x + '/', get_years(\"hep-th-abs/\")))\n",
    "        for directory in dirs:\n",
    "            for abs_name in get_abstracts(directory):\n",
    "                # get path and get author ids for this abstract\n",
    "                path = directory + abs_name\n",
    "                # note that if author_ids doesn't return anything, this means\n",
    "                # we couldn't parse the authors for this paper, and hence, \n",
    "                # we do not include this abstract in the abstract map. \n",
    "                author_ids = get_abstract_author_ids(path, author_dict)\n",
    "                # strip the .abs from abs_name to get abs_id\n",
    "                abs_id = abs_name.split('.')[0]\n",
    "                # write abstract id followed list of authors ids to file.\n",
    "                f.write(abs_id + \"\\t\" + str(author_ids) + \"\\n\")\n",
    "                \n",
    "# loads the abstract_map and returns a list of lists, every list contains the authors who collaborated together \n",
    "# on a specific paper. \n",
    "def get_col_abs_list():\n",
    "    abs_collab_list = []\n",
    "    with open(\"abstract_map.txt\", 'r') as f:\n",
    "        for line in f:\n",
    "            str_author_ids = line.split(\"\\t\")[1]\n",
    "            author_ids = ast.literal_eval(str_author_ids)\n",
    "            abs_collab_list.append(author_ids)\n",
    "    return abs_collab_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_abstract_map(author_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"now get the collaboration map\"\"\"\n",
    "\n",
    "# a class to store info about the collaboration of some pair of authors. \n",
    "class collab_edge:\n",
    "    def __init__(self, pair):\n",
    "        self.pair = pair\n",
    "        self.count = 1\n",
    "        \n",
    "    def inc_count(self):\n",
    "        self.count += 1\n",
    "        \n",
    "    # return true, if pair is identical.\n",
    "    def is_identical(self, pair):\n",
    "        return self.pair[0] in pair and self.pair[1] in pair\n",
    "\n",
    "\n",
    "# takes  a list of authors from a paper and produces a list of sets of all possible pairs\n",
    "def create_collab_objs(abs_collab):\n",
    "    collab_objs = []\n",
    "    # get all possible pairs and convert to sets (so order become irrelevant)\n",
    "    for abstr in abs_collab:\n",
    "        # if only one, or no authors, disregard it.\n",
    "        if len(abstr) <= 1:\n",
    "            continue\n",
    "        # get all possible pairs from this abstr\n",
    "        abs_pairs = list(itertools.combinations(abstr, 2))\n",
    "        for abs_pair in abs_pairs:\n",
    "            no_match = True\n",
    "            for collab_obj in collab_objs:\n",
    "                if collab_obj.is_identical(abs_pair):\n",
    "                    collab_obj.inc_count()\n",
    "                    no_match = False\n",
    "                    break\n",
    "            # didn't match any object, create new object\n",
    "            if no_match: \n",
    "                collab_objs.append(collab_edge(abs_pair))\n",
    "    return collab_objs\n",
    "\n",
    "\n",
    "# write collab_objs to file with weights\n",
    "def create_weighted_collab_graph(collab_objs):\n",
    "    with open(\"collab_graph_weighted.txt\", 'a') as f:\n",
    "        for obj in collab_objs:\n",
    "            f.write(obj.pair[0] + \" \" + obj.pair[1] + \"\\t\" + str(obj.count) + \"\\n\")\n",
    "# write collab objs to file without weights\n",
    "def create_unweighted_collab_graph(collab_objs):\n",
    "    with open(\"collab_graph_unweighted.txt\", 'a') as f:\n",
    "        for obj in collab_objs:\n",
    "            f.write(obj.pair[0] + \" \" + obj.pair[1] + \"\\n\")\n",
    "\n",
    "            \n",
    "col_abs_list = get_col_abs_list()\n",
    "collab_objets = create_collab_objs(col_abs_list)            \n",
    "create_weighted_collab_graph(collab_objets)\n",
    "create_unweighted_collab_graph(collab_objets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_productivity_of_author(author_id, col_abs_list):\n",
    "    count = 0\n",
    "    for lst in col_abs_list:\n",
    "        if author_id in lst:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def get_productivity_of_authors(author_dict, col_abs_list):\n",
    "    author_prods = []\n",
    "    for author_id in author_dict.keys():\n",
    "        productivity = get_productivity_of_author(author_id, col_abs_list)\n",
    "        author_prod = (author_id, productivity)\n",
    "        author_prods.append(author_prod)\n",
    "    return author_prods\n",
    "\n",
    "# write author productivity to file\n",
    "def create_productivity_map(author_prods):\n",
    "    with open(\"/productivity_data/raw_productivity\", 'a') as f:\n",
    "        for author_prod in author_prods:\n",
    "            f.write(author_prod[0] + \" \" + str(author_prod[1]) + \"\\n\")\n",
    "            \n",
    "author_prods = get_productivity_of_authors(author_dict, col_abs_list)\n",
    "create_productivity_map(author_prods)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:iaml]",
   "language": "python",
   "name": "conda-env-iaml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
